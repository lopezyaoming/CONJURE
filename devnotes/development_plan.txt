# CONJURE - Step-by-Step Development Plan

This document outlines a logical and sequential development path for the CONJURE project. Each phase builds upon the last, ensuring that we have a working, testable system at every stage.

---

### **Phase 1: Core Interaction Loop (✅ Completed)**

The goal of this phase was to establish the fundamental real-time link between hand gestures and mesh deformation.

-   [x] **Setup Project Structure:** Create the initial folders and files (`launcher`, `blender`, `data`, etc.).
-   [x] **Implement Hand Tracker:** Create `hand_tracker.py` to capture webcam feed, use Mediapipe to detect hands, and write data to `fingertips.json`.
-   [x] **Implement Blender Script:** Create `conjure_blender.py` to read `fingertips.json`, plot visual markers for fingertips, and deform a target mesh.
-   [x] **Establish I/O Bridge:** Successfully link the two scripts via the `fingertips.json` file.
-   [x] **Refine Interaction:** Implement camera-relative mapping, refresh rate optimization, and visual smoothing for an intuitive user experience.

---

### **Phase 2: Expanding Gestures & Blender Actions**

The goal of this phase is to move beyond simple deformation and implement the other core interaction modes described in `masterPrompt.txt`.

1.  **Implement Rotation:**
    -   [x] In `hand_tracker.py`: Add gesture detection for rotation (thumb + middle/ring finger).
    -   [x] In `hand_tracker.py`: Add `"rotate_z"` and `"rotate_y"` commands to the JSON output.
    -   [x] In `conjure_blender.py`: When rotation commands are received, orbit the `GestureCamera` around the `DeformableMesh`.

2.  **Implement Brush System:**
    -   [x] In `hand_tracker.py`: Add gesture for cycling brushes (left hand thumb + middle finger).
    -   [x] In `hand_tracker.py`: Add a `"cycle_brush"` command to the JSON output.
    -   [x] In `conjure_blender.py`: Implement `PINCH`, `GRAB`, and `SMOOTH` brush types.
    -   [x] In `conjure_blender.py`: Add UI text to display the current brush.
    -   [x] In `conjure_blender.py`: Implement logic for `GRAB` (moving mesh with hand) and `SMOOTH` (averaging vertices).

3.  **Implement History/Undo:**
    -   [x] In `hand_tracker.py`: Add gesture for "rewind" (thumb + pinky finger).
    -   [x] In `conjure_blender.py`: Implement a history buffer (`deque`) to store mesh states.
    -   [x] In `conjure_blender.py`: When `"rewind"` command is received, pop the last state from the buffer and apply it to the mesh.

---

### **Phase 2.5: Ergonomics & Feel Tuning (✅ Completed)**

This phase involved a deep, iterative refinement of the core user experience based on hands-on testing. The focus was on making the tool feel intuitive, responsive, and natural.

-   [x] **Overhaul Orbit Controls:** Replaced the clunky, modal rotation system with a fluid, direct-manipulation orbit control that is locked to the world origin.
-   [x] **Expand Brush Palette:** Added the `INFLATE` and `FLATTEN` brushes to the sculpting toolkit for more advanced shape control.
-   [x] **Implement Brush Radius Cycling:** Created a three-tier radius system (`SMALL`, `MEDIUM`, `LARGE`) and mapped it to a dedicated gesture.
-   [x] **Implement Brush Falloff:** Added smooth, distance-based falloff to all sculpting brushes to create natural, organic results and eliminate harsh edges.
-   [x] **Implement Surface Snapping:** Added camera-based raycasting to project the fingertip cursor onto the mesh surface, preventing it from getting lost inside the model and providing a tactile, "snap-to-surface" feel.
-   [x] **Refine Gesture Ergonomics:** Re-mapped gestures for a more logical separation of concerns between the left and right hands.
-   [x] **Improve Core Mechanics:** Upgraded the rewind feature to be a continuous, press-and-hold action for much faster undoing.
-   [x] **Comprehensive Calibration:** Performed numerous micro-calibrations of brush strengths, radii, falloffs, and sensitivities to perfect the tool's "feel".
-   [x] **Final Stability Pass:** Implemented multiple fixes for visual and interaction stability, including robust context handling, a visibility grace period for markers, and a camera-based offset to eliminate z-fighting.

---

### **Phase 3: The `Launcher` as an Orchestrator (✅ Completed)**

The goal of this phase is to build out the main application that manages all the subprocesses, as described in `README.md`.

1.  **Create the Main Executable:**
    -   [x] In `launcher/main.py`: Write the script that will serve as the application's entry point.
    -   [x] In `launcher/subprocess_manager.py`: Implement functions to start and stop the `hand_tracker.py` script as a background process.
    -   [x] In `launcher/subprocess_manager.py`: Add logic to launch Blender with the `conjure_blender.py` script automatically.
    -   [x] In `launcher/config.py`: Add user-specific path for the Blender executable.

2.  **Implement State Management:**
    -   [x] In `launcher/state_manager.py`: Create a class to read and write to `state.json`.
    -   [x] The main loop in `launcher/main.py` will now be driven by this state, which will eventually coordinate Blender, ComfyUI, and the Agent.

---

### **Phase 4: ComfyUI Integration & UI Controls (✅ Completed)**

The goal is to get the first end-to-end generative loop working: from a Blender render to AI image options.

1.  **Implement Generative UI Controls in Blender:**
    -   [x] In `scripts/addons/conjure/panel_ui.py`: Add a **"Generate Concepts"** button to the CONJURE UI panel.
    -   [x] In `scripts/addons/conjure/ops_io.py`: Create a placeholder operator that is called by the button.
    -   [x] In `scripts/addons/conjure/ops_io.py`: Implement the operator logic to render an image from the `GestureCamera` to the correct data path.
    -   [x] In `scripts/addons/conjure/ops_io.py`: Implement logic to update `state.json` to notify the launcher of a new generation request.

2.  **Implement ComfyUI Wrapper:**
    -   [x] In `comfyui/api_wrapper.py`: Write functions to connect to the ComfyUI server's API and trigger a workflow execution.
    -   [x] Ensure the `promptMaker.json` workflow is correctly configured to accept an input image.

3.  **Create Basic GUI:**
    -   [x] In `launcher/gui.py`: Using PyQt6, create a very simple, non-interactive window that can display three images side-by-side.
    -   [x] In `launcher/main.py`: When the Blender render is complete (by checking for the file's existence), use the `api_wrapper` to send the image to ComfyUI.
    -   [x] When ComfyUI returns the 3 generated images, display them in the GUI window.

---

### **Phase 5: Full 2D-to-3D Pipeline (✅ Completed)**

The goal is to implement the full multi-view 2D-to-3D generation pipeline, including the mesh history system.

1.  **Implement Multi-View Rendering:**
    -   [x] In `scripts/addons/conjure/ops_io.py`: Create a function that, when triggered, renders the `DeformableMesh` from 6 specific angles using the `MVCamera` and saves them to `data/generated_images/multiviewRender/`.
    -   [x] This is triggered by selecting one of the 3 concept images.

2.  **Trigger `mv23D` Workflow:**
    -   [x] In `launcher/main.py`: Once the multi-view images are generated, use the `api_wrapper` to automatically trigger the `mv23D.json` workflow in ComfyUI.
    -   [x] In `launcher/main.py`: Update `state.json` to signal to Blender that a new model is ready for import.

3.  **Implement Mesh History & Import System:**
    -   [x] In `scripts/addons/conjure/operator_main.py`: The main operator will check the state file for an `import_request`.
    -   [x] In `scripts/addons/conjure/ops_io.py`: The `import_model` operator will be triggered.
    -   [x] The operator will create a "HISTORY" collection if one doesn't exist.
    -   [x] It will rename the current "Mesh" to "Mesh.XXX", move it to the HISTORY collection, and hide it.
    -   [x] It will then import the new `copyMesh.glb` from the dedicated ComfyUI output folder into `data/generated_models/`.
    -   [x] Finally, it will rename the newly imported object to "Mesh", making it the active sculptable object.

---

### **Phase 6: Conversational Agent Integration (Design) (✅ Completed)**

The goal of this phase was to research the best approach for voice integration and to design the agent's core logic, personality, and technical interface.

-   [x] **Research & Document ElevenLabs API:** Analyzed the `ElevenLabs Conversational AI` platform and documented a detailed integration strategy in `devnotes/elevenLabsnotes.txt`.
-   [x] **Craft Agent System Prompt:** Created a comprehensive system prompt (`devnotes/agentPrompt.txt`) to define the agent's persona, rules, and step-by-step conversational flow for the entire creative process.
-   [x] **Define Agent Toolset & Data Structure:** Specified a structured toolset (`devnotes/agentToolset.txt`) and a clear JSON output format, defining exactly how the agent is to interact with the CONJURE backend via tool calls.

---

### **Phase 7: Agent Implementation (✅ Completed)**

The goal is to bring the fully-designed conversational agent to life by implementing the connection to the ElevenLabs API and wiring its commands into our state-driven system.

-   **1. Configure the Cloud Agent:**
    -   [x] In the ElevenLabs dashboard, create the Conversational AI Agent.
    -   [x] Configure it with a voice, an LLM (e.g., GPT-4o), and copy the contents of `devnotes/agentPrompt.txt` into its system prompt.
    -   [x] Implement the tools defined in `devnotes/agentToolset.txt` within the ElevenLabs agent's configuration.

-   **2. Implement Real-Time Audio & API Connection:**
    -   [x] In `launcher/agent_api.py`, implement the WebSocket (`WSS`) connection to the ElevenLabs agent.
    -   [x] Add microphone audio capture (e.g., using `PyAudio`) and stream the live audio to the agent.
    -   [x] Add logic to receive the agent's TTS audio response and play it back to the user.

-   **3. Implement Tool & Prompt Handling:**
    -   [x] In `launcher/agent_api.py`, create a listener for the agent's structured JSON output.
    -   [x] Write a parser for the `instruction` object. When a tool call is received, use `state_manager.py` to update `state.json` with the corresponding command.
    -   [x] Write the `user_prompt` string from the agent's JSON output to the `data/generated_text/userPrompt.txt` file.

-   **4. Integrate with Main Application Loop:**
    -   [x] In `launcher/main.py`, instantiate and start the agent connection.
    -   [x] Ensure the main loop correctly reacts to the commands in `state.json` as they are now being triggered by the agent.

---

### **Phase 8: Overlay Transparent GUI (✅ Completed)**

The goal is to build out the final user interface for displaying AI-generated images, status information, and a chat log.

-   [x] **Flesh out GUI:** Build the complete, interactive GUI in `launcher/gui.py` using PyQt6.
-   [x] **Add Chat/Log Box:** Implement a text area to display agent messages and a log of system events.
-   [x] **Add Status Indicators:** Create visual elements to show when the system is busy (e.g., "Generating...", "Importing...") and to display the active brush type and radius.

---

### **Phase 9: CGAL Integration (Mesh Segmentation)**

The goal is to integrate the CGAL-based mesh segmenter to allow for part-based editing.

-   [ ] **Create Segmenter Wrapper:** Implement the subprocess call in `cgal/segmenter_wrapper.py` to run the C++ executable.
-   [ ] **Add UI Control:** Add a "Segment Mesh" button to the Blender UI panel.
-   [ ] **Implement I/O:** Add operator logic in Blender to export the current mesh as a `.ply` file for the segmenter.
-   [ ] **Implement Parser:** Add logic to re-import the vertex-colored `.ply` file and parse the color groups into selectable segments in Blender.
-   [ ] **Isolate Segment:** Develop the logic to select a segment and isolate it for a focused refinement loop. 