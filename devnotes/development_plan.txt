# CONJURE - Step-by-Step Development Plan

This document outlines a logical and sequential development path for the CONJURE project. Each phase builds upon the last, ensuring that we have a working, testable system at every stage.

---

### **Phase 1: Core Interaction Loop (✅ Completed)**

The goal of this phase was to establish the fundamental real-time link between hand gestures and mesh deformation.

-   [x] **Setup Project Structure:** Create the initial folders and files (`launcher`, `blender`, `data`, etc.).
-   [x] **Implement Hand Tracker:** Create `hand_tracker.py` to capture webcam feed, use Mediapipe to detect hands, and write data to `fingertips.json`.
-   [x] **Implement Blender Script:** Create `conjure_blender.py` to read `fingertips.json`, plot visual markers for fingertips, and deform a target mesh.
-   [x] **Establish I/O Bridge:** Successfully link the two scripts via the `fingertips.json` file.
-   [x] **Refine Interaction:** Implement camera-relative mapping, refresh rate optimization, and visual smoothing for an intuitive user experience.

---

### **Phase 2: Expanding Gestures & Blender Actions**

The goal of this phase is to move beyond simple deformation and implement the other core interaction modes described in `masterPrompt.txt`.

1.  **Implement Rotation:**
    -   [x] In `hand_tracker.py`: Add gesture detection for rotation (thumb + middle/ring finger).
    -   [x] In `hand_tracker.py`: Add `"rotate_z"` and `"rotate_y"` commands to the JSON output.
    -   [x] In `conjure_blender.py`: When rotation commands are received, orbit the `GestureCamera` around the `DeformableMesh`.

2.  **Implement Brush System:**
    -   [x] In `hand_tracker.py`: Add gesture for cycling brushes (left hand thumb + middle finger).
    -   [x] In `hand_tracker.py`: Add a `"cycle_brush"` command to the JSON output.
    -   [x] In `conjure_blender.py`: Implement `PINCH`, `GRAB`, and `SMOOTH` brush types.
    -   [x] In `conjure_blender.py`: Add UI text to display the current brush.
    -   [x] In `conjure_blender.py`: Implement logic for `GRAB` (moving mesh with hand) and `SMOOTH` (averaging vertices).

3.  **Implement History/Undo:**
    -   [x] In `hand_tracker.py`: Add gesture for "rewind" (thumb + pinky finger).
    -   [x] In `conjure_blender.py`: Implement a history buffer (`deque`) to store mesh states.
    -   [x] In `conjure_blender.py`: When `"rewind"` command is received, pop the last state from the buffer and apply it to the mesh.

---

### **Phase 2.5: Ergonomics & Feel Tuning (✅ Completed)**

This phase involved a deep, iterative refinement of the core user experience based on hands-on testing. The focus was on making the tool feel intuitive, responsive, and natural.

-   [x] **Overhaul Orbit Controls:** Replaced the clunky, modal rotation system with a fluid, direct-manipulation orbit control that is locked to the world origin.
-   [x] **Expand Brush Palette:** Added the `INFLATE` and `FLATTEN` brushes to the sculpting toolkit for more advanced shape control.
-   [x] **Implement Brush Radius Cycling:** Created a three-tier radius system (`SMALL`, `MEDIUM`, `LARGE`) and mapped it to a dedicated gesture.
-   [x] **Implement Brush Falloff:** Added smooth, distance-based falloff to all sculpting brushes to create natural, organic results and eliminate harsh edges.
-   [x] **Implement Surface Snapping:** Added camera-based raycasting to project the fingertip cursor onto the mesh surface, preventing it from getting lost inside the model and providing a tactile, "snap-to-surface" feel.
-   [x] **Refine Gesture Ergonomics:** Re-mapped gestures for a more logical separation of concerns between the left and right hands.
-   [x] **Improve Core Mechanics:** Upgraded the rewind feature to be a continuous, press-and-hold action for much faster undoing.
-   [x] **Comprehensive Calibration:** Performed numerous micro-calibrations of brush strengths, radii, falloffs, and sensitivities to perfect the tool's "feel".
-   [x] **Final Stability Pass:** Implemented multiple fixes for visual and interaction stability, including robust context handling, a visibility grace period for markers, and a camera-based offset to eliminate z-fighting.

---

### **Phase 3: The `Launcher` as an Orchestrator (✅ Completed)**

The goal of this phase is to build out the main application that manages all the subprocesses, as described in `README.md`.

1.  **Create the Main Executable:**
    -   [x] In `launcher/main.py`: Write the script that will serve as the application's entry point.
    -   [x] In `launcher/subprocess_manager.py`: Implement functions to start and stop the `hand_tracker.py` script as a background process.
    -   [x] In `launcher/subprocess_manager.py`: Add logic to launch Blender with the `conjure_blender.py` script automatically.
    -   [x] In `launcher/config.py`: Add user-specific path for the Blender executable.

2.  **Implement State Management:**
    -   [x] In `launcher/state_manager.py`: Create a class to read and write to `state.json`.
    -   [x] The main loop in `launcher/main.py` will now be driven by this state, which will eventually coordinate Blender, ComfyUI, and the Agent.

---

### **Phase 4: ComfyUI Integration & UI Controls**

The goal is to get the first end-to-end generative loop working: from a Blender render to AI image options.

1.  **Implement Generative UI Controls in Blender:**
    -   In `conjure_blender.py`: Add a **"Render"** button to the CONJURE UI panel.
    -   This button will render an image from the `GestureCamera` perspective and save it to `data/generated_images/gestureCamera/render.png`.
    -   In `conjure_blender.py`: Add a **"Generate Model"** button to the panel.
    -   This button will eventually trigger the entire ComfyUI pipeline via the `state_manager`.

2.  **Implement ComfyUI Wrapper:**
    -   In `comfyui/api_wrapper.py`: Write functions to connect to the ComfyUI server's API and trigger a workflow execution.
    -   Ensure the `promptMaker.json` workflow is correctly configured to accept an input image.

3.  **Create Basic GUI:**
    -   In `launcher/gui.py`: Using PyQt6, create a very simple, non-interactive window that can display three images side-by-side.
    -   In `launcher/main.py`: When the Blender render is complete (by checking for the file's existence), use the `api_wrapper` to send the image to ComfyUI.
    -   When ComfyUI returns the 3 generated images, display them in the GUI window.

---

### **Phase 5: ComfyUI Integration (3D `mvto3D` Workflow)**

The goal is to implement the full multi-view 2D-to-3D generation pipeline.

1.  **Implement Multi-View Rendering:**
    -   In `conjure_blender.py`: Create a function that, when triggered, renders the `DeformableMesh` from 6 specific angles using the `MVCamera` and saves them to `data/generated_images/multiviewRender/`.
    -   This should be triggered by a specific user action (e.g., selecting one of the 3 images from the GUI in Phase 4).

2.  **Trigger `mvto3D` Workflow:**
    -   In `launcher/main.py`: Once all 6 renders are present, use the `api_wrapper` to trigger the `mvto3D` workflow in ComfyUI, passing the images and a selected prompt.

3.  **Import Generated Mesh:**
    -   In `conjure_blender.py`: Add a "file watcher" that periodically checks for the `genMesh.glb` file in `data/generated_models/`.
    -   When a new mesh is detected, automatically import it, replace the current `DeformableMesh`, and normalize it to fit the bounding box.

---

### **Phase 6 & Beyond: Agent, Segmentation, and Polish**

Once the core generative loop is working, we can add the final layers of functionality.

1.  **Conversational Agent (`agent_api.py`):** Integrate ElevenLabs for voice-to-text and text-to-speech to generate prompts and provide narrative feedback.
2.  **CGAL Segmenter:** Implement the subprocess call to the C++ segmenter and the Blender logic to parse the resulting vertex-colored mesh.
3.  **Full GUI Implementation:** Build out the complete, interactive GUI with chat boxes, loading screens, and segment selection.
4.  **Refinement and Testing:** Thoroughly test each phase and optimize for performance and usability. 