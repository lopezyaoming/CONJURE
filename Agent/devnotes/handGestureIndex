right hand :


finger touching will be managed by calculating the distance of each finger and giving a margin of error for the distance to be effective
When fingers touch: 
Thumb and index finger: activate deformation. the deformation is only active as long as index and thumb are touching. (its a boolean, where there is an active/inactive deformation)
thumb and middle finger: rotate around Z axis (so it rotates horizontally). it's a boolean switch, and the rotation/second is a variable rotation_speed
Thumb and ring finger: rotate around Y axis (so it rotates vertically) it's a boolean switch, and the rotation/second is the same variable (rotation_speed)
thumb and pinky finger: spawns an object, in this case a cube. the size of the cube is determined by the distance of the thumb and pinky in space. so it scales as the user moves it's fingers. the cube is positioned and boolean unioned with mesh. 

we will use mediapipe gesture recognition to add more functions. https://ai.google.dev/edge/mediapipe/solutions/vision/gesture_recognizer#configurations_options
gesture recognizer label 1 - Closed_Fist will be used to directly jump to the next step in the process, so if an user is gesture modelling but feels that it is complete, he can use this gesture to quickly request for the next step to begin (for example, rendering and creating new options)